{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for a simple neural network\n",
    "\n",
    "## Trump by maximum color (2 colors)\n",
    "\n",
    "The inputs to the network are the number of cards of each color. The network should learn to select the color with the largest number of cards of that color.\n",
    "\n",
    "For a simple example, let us assume that there are 5 cards in total for a player and only 2 colors.\n",
    "\n",
    "### Libraries\n",
    "\n",
    "We use the keras library for building, training and evaluating the network. A tutorial for keras can be found on (https://keras.io/) or https://www.tensorflow.org/guide/keras. There are different implementations of keras, here I will use the one build on tensorflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output function\n",
    "\n",
    "We have to encode the output somehow, for two classes, the simplest solution is a single variable that should be 0 if there are more cards of color 0 and 1 if there are more cards of color 1.\n",
    "\n",
    "### Training and label data.\n",
    "\n",
    "So we can prepare some training data. In this simple case, all the possible configurations are actually known.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 5.]\n",
      " [1. 4.]\n",
      " [2. 3.]\n",
      " [3. 2.]\n",
      " [4. 1.]\n",
      " [5. 0.]]\n",
      "[1. 1. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([\n",
    "    [0, 5],\n",
    "    [1, 4],\n",
    "    [2, 3],\n",
    "    [3, 2],\n",
    "    [4, 1],\n",
    "    [5, 0],\n",
    "], dtype=np.float32)\n",
    "y_train = np.array([1, 1, 1, 0, 0, 0,], dtype=np.float32)\n",
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation\n",
    "\n",
    "Input data can have different ranges. It is always a good idea (in other words absolutely essential) to normalize the input data. This is usually done into the range 0..1 or -1..1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  1. ]\n",
      " [0.2 0.8]\n",
      " [0.4 0.6]\n",
      " [0.6 0.4]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train / 5.0\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first network.\n",
    "\n",
    "We will start with a very simple network, where we connect the inputs directly to the output. So there will be 2 variables, the weights for the connection and the bias. The output function is a sigmoid, which takes values between 0 and 1.\n",
    "\n",
    "With keras, we first have to create the type of model we want (Sequential), and can then add layers. In the tensorflow implementation, we have to add the input_shape parameter in the first layer to tell it the format of the input. This does not include the batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lucar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()  #Feed Forward Network\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid', input_shape=[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to compile the model and tell it what loss function and optimizer we want to have. We will take a mean squared error for loss function first. (This is actually not optimal and will be corrected in an exercise).\n",
    "\n",
    "Besides the loss, we usually want to look at some metrics. Here we choose accuracy, that measures how often the network makes the correct decision (see last lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lucar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print some details about the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[array([[0.42226315],\n",
      "       [0.19008565]], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "print(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either train one batch, or we can use fit to train repeatedly. The result from the training is the loss function and the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lucar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2731249, 0.5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_on_batch(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try to fit the data in minibatches multiple times. This will calculate the weights, so as to minimize the loss. We might not always get a good result in the first try and even this very simple network seems to need a large number of training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 0s 29ms/sample - loss: 0.2730 - acc: 0.5000\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 325us/sample - loss: 0.2729 - acc: 0.5000\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2727 - acc: 0.5000\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2726 - acc: 0.5000\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2725 - acc: 0.5000\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2723 - acc: 0.5000\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2722 - acc: 0.5000\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2721 - acc: 0.5000\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 325us/sample - loss: 0.2719 - acc: 0.5000\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2718 - acc: 0.5000\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 167us/sample - loss: 0.2717 - acc: 0.5000\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 331us/sample - loss: 0.2715 - acc: 0.5000\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2714 - acc: 0.5000\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 327us/sample - loss: 0.2713 - acc: 0.5000\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 338us/sample - loss: 0.2711 - acc: 0.5000\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 161us/sample - loss: 0.2710 - acc: 0.5000\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2709 - acc: 0.5000\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2707 - acc: 0.5000\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2706 - acc: 0.5000\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2705 - acc: 0.5000\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 167us/sample - loss: 0.2703 - acc: 0.5000\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2702 - acc: 0.5000\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 326us/sample - loss: 0.2701 - acc: 0.5000\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2699 - acc: 0.5000\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2698 - acc: 0.5000\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2697 - acc: 0.5000\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2695 - acc: 0.5000\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2694 - acc: 0.5000\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2693 - acc: 0.5000\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2691 - acc: 0.5000\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 160us/sample - loss: 0.2690 - acc: 0.5000\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2689 - acc: 0.5000\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 324us/sample - loss: 0.2688 - acc: 0.5000\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2686 - acc: 0.5000\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2685 - acc: 0.5000\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2684 - acc: 0.5000\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2682 - acc: 0.5000\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2681 - acc: 0.5000\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 158us/sample - loss: 0.2680 - acc: 0.5000\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2678 - acc: 0.5000\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2677 - acc: 0.5000\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2676 - acc: 0.5000\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2674 - acc: 0.5000\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 338us/sample - loss: 0.2673 - acc: 0.5000\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 160us/sample - loss: 0.2672 - acc: 0.5000\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2670 - acc: 0.5000\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2669 - acc: 0.5000\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 167us/sample - loss: 0.2668 - acc: 0.5000\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2667 - acc: 0.5000\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2665 - acc: 0.5000\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2664 - acc: 0.5000\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2663 - acc: 0.5000\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2661 - acc: 0.5000\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2660 - acc: 0.5000\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2659 - acc: 0.5000\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2657 - acc: 0.5000\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2656 - acc: 0.5000\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 162us/sample - loss: 0.2655 - acc: 0.5000\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2654 - acc: 0.5000\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2652 - acc: 0.5000\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2651 - acc: 0.5000\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2650 - acc: 0.5000\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 172us/sample - loss: 0.2648 - acc: 0.5000\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2647 - acc: 0.5000\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 339us/sample - loss: 0.2646 - acc: 0.5000\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2645 - acc: 0.5000\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2643 - acc: 0.5000\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2642 - acc: 0.5000\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2641 - acc: 0.5000\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 167us/sample - loss: 0.2639 - acc: 0.5000\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2638 - acc: 0.5000\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2637 - acc: 0.5000\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2636 - acc: 0.5000\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 328us/sample - loss: 0.2634 - acc: 0.5000\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2633 - acc: 0.5000\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2632 - acc: 0.5000\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2630 - acc: 0.5000\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2629 - acc: 0.5000\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2628 - acc: 0.5000\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2627 - acc: 0.5000\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 329us/sample - loss: 0.2625 - acc: 0.5000\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 161us/sample - loss: 0.2624 - acc: 0.5000\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2623 - acc: 0.5000\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2621 - acc: 0.5000\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2620 - acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2619 - acc: 0.5000\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2618 - acc: 0.5000\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 167us/sample - loss: 0.2616 - acc: 0.5000\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2615 - acc: 0.5000\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2614 - acc: 0.5000\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 338us/sample - loss: 0.2613 - acc: 0.5000\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2611 - acc: 0.5000\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2610 - acc: 0.5000\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2609 - acc: 0.5000\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2608 - acc: 0.5000\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2606 - acc: 0.5000\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 331us/sample - loss: 0.2605 - acc: 0.5000\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2604 - acc: 0.5000\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2602 - acc: 0.5000\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2601 - acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x169d6d6f7b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100, batch_size=6) # Epoche: wie viele male durch die ganzen Daten; batch_size: wie gross sollen die Datensätze sein welche durchs Netz geschickt werden (Mini-Batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predict the values from the training value. Why are the results floating point number and not 0 or 1? Does the result seem likely?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5540702 ],\n",
       "       [0.5579984 ],\n",
       "       [0.56191945],\n",
       "       [0.56583273],\n",
       "       [0.56973785],\n",
       "       [0.57363427]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the found weights for each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense\n",
      "[array([[0.32953602],\n",
      "       [0.24997126]], dtype=float32), array([-0.03284146], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    print(layer.name)\n",
    "    print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we find the actual predictions? We use a threshold on the output of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train) > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A larger network\n",
    "\n",
    "Lets try a more complicated network with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(2, activation='relu', input_shape=[2]))\n",
    "model.add(keras.layers.Dense(2, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train it again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 0s 45ms/sample - loss: 0.3778 - acc: 0.5000\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3769 - acc: 0.5000\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 328us/sample - loss: 0.3760 - acc: 0.5000\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 328us/sample - loss: 0.3751 - acc: 0.5000\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 330us/sample - loss: 0.3742 - acc: 0.5000\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.3733 - acc: 0.5000\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3724 - acc: 0.5000\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3715 - acc: 0.5000\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.3706 - acc: 0.5000\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 329us/sample - loss: 0.3697 - acc: 0.5000\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3688 - acc: 0.5000\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 338us/sample - loss: 0.3679 - acc: 0.5000\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.3670 - acc: 0.5000\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3661 - acc: 0.5000\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3652 - acc: 0.5000\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3644 - acc: 0.5000\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 328us/sample - loss: 0.3635 - acc: 0.5000\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3626 - acc: 0.5000\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 498us/sample - loss: 0.3617 - acc: 0.5000\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3608 - acc: 0.5000\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3600 - acc: 0.5000\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3591 - acc: 0.5000\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 337us/sample - loss: 0.3582 - acc: 0.5000\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3573 - acc: 0.5000\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3565 - acc: 0.5000\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3556 - acc: 0.5000\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3547 - acc: 0.5000\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 327us/sample - loss: 0.3539 - acc: 0.5000\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 331us/sample - loss: 0.3530 - acc: 0.5000\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 328us/sample - loss: 0.3522 - acc: 0.5000\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3513 - acc: 0.5000\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3505 - acc: 0.5000\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3496 - acc: 0.5000\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 328us/sample - loss: 0.3488 - acc: 0.5000\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3479 - acc: 0.5000\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3471 - acc: 0.5000\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.3463 - acc: 0.5000\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 336us/sample - loss: 0.3454 - acc: 0.5000\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3446 - acc: 0.5000\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3438 - acc: 0.5000\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3430 - acc: 0.5000\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3421 - acc: 0.5000\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3413 - acc: 0.5000\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.3405 - acc: 0.5000\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3397 - acc: 0.5000\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3389 - acc: 0.5000\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 326us/sample - loss: 0.3377 - acc: 0.5000\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 336us/sample - loss: 0.3366 - acc: 0.5000\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 167us/sample - loss: 0.3355 - acc: 0.5000\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 326us/sample - loss: 0.3344 - acc: 0.5000\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3333 - acc: 0.5000\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3321 - acc: 0.5000\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3310 - acc: 0.5000\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3299 - acc: 0.5000\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 324us/sample - loss: 0.3288 - acc: 0.5000\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3278 - acc: 0.5000\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3267 - acc: 0.5000\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3256 - acc: 0.5000\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3245 - acc: 0.5000\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3234 - acc: 0.5000\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3224 - acc: 0.5000\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3213 - acc: 0.5000\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 338us/sample - loss: 0.3202 - acc: 0.5000\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 158us/sample - loss: 0.3192 - acc: 0.5000\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.3181 - acc: 0.5000\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3171 - acc: 0.5000\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 165us/sample - loss: 0.3161 - acc: 0.5000\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.3150 - acc: 0.5000\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 161us/sample - loss: 0.3140 - acc: 0.5000\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3130 - acc: 0.5000\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.3119 - acc: 0.5000\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 167us/sample - loss: 0.3109 - acc: 0.5000\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3099 - acc: 0.5000\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3089 - acc: 0.5000\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3079 - acc: 0.5000\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3069 - acc: 0.5000\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3059 - acc: 0.5000\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3049 - acc: 0.5000\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3039 - acc: 0.5000\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3030 - acc: 0.5000\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3020 - acc: 0.5000\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3010 - acc: 0.5000\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3001 - acc: 0.5000\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2991 - acc: 0.5000\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2982 - acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2972 - acc: 0.5000\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2963 - acc: 0.5000\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2953 - acc: 0.5000\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2944 - acc: 0.5000\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2935 - acc: 0.5000\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2925 - acc: 0.5000\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2916 - acc: 0.5000\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2907 - acc: 0.5000\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2898 - acc: 0.5000\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2889 - acc: 0.5000\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 160us/sample - loss: 0.2880 - acc: 0.5000\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 324us/sample - loss: 0.2871 - acc: 0.5000\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2862 - acc: 0.5000\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2854 - acc: 0.5000\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2845 - acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x169d577b438>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100, batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not necessarly better, how does the prediction look now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33950227],\n",
       "       [0.40222698],\n",
       "       [0.46638152],\n",
       "       [0.47019863],\n",
       "       [0.4563381 ],\n",
       "       [0.44039917]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.layers.core.Dense object at 0x00000169D6EB34A8>\n",
      "[array([[-0.30224428,  0.96389234],\n",
      "       [ 0.9328564 , -0.09588786]], dtype=float32), array([-0.15454654,  0.01976662], dtype=float32)]\n",
      "<tensorflow.python.keras.layers.core.Dense object at 0x00000169D6EB3630>\n",
      "[array([[ 0.80477417,  0.2516636 ],\n",
      "       [-0.17768416,  0.32756498]], dtype=float32), array([-0.14752015, -0.02973005], dtype=float32)]\n",
      "<tensorflow.python.keras.layers.core.Dense object at 0x00000169D6EB3898>\n",
      "[array([[-1.1344988],\n",
      "       [-0.9282603]], dtype=float32), array([0.03195744], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    print(layer)\n",
    "    print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger network, does not seem to work better as the simpler one. Or is it maybe not large enough?\n",
    "\n",
    "The problem is not the network, but the data, we just do not have enough data. So lets try to make up some more data artificially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.random.random(size=(10000,2))\n",
    "y_new = np.zeros(10000, dtype=np.float32)\n",
    "condition = (x_new[:,1] > x_new[:,0])\n",
    "y_new[condition] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2536 - acc: 0.5565\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2425 - acc: 0.6022\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2378 - acc: 0.6458\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2344 - acc: 0.6801\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2315 - acc: 0.7117\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2286 - acc: 0.7227\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2257 - acc: 0.7321\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2226 - acc: 0.7407\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2194 - acc: 0.7526\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2161 - acc: 0.7574\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2125 - acc: 0.7670\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2087 - acc: 0.7744\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2047 - acc: 0.7831\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2004 - acc: 0.7899\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1960 - acc: 0.8008\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1914 - acc: 0.8113\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1866 - acc: 0.8171\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1817 - acc: 0.8267\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1767 - acc: 0.8353\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1716 - acc: 0.8431\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1665 - acc: 0.8511\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1613 - acc: 0.8592\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1561 - acc: 0.8661\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1510 - acc: 0.8734\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1460 - acc: 0.8817\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1410 - acc: 0.8892\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1362 - acc: 0.8954\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1315 - acc: 0.9015\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1269 - acc: 0.9065\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1225 - acc: 0.9147\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1182 - acc: 0.9200\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1140 - acc: 0.9248\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1101 - acc: 0.9299\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1063 - acc: 0.9357\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1027 - acc: 0.9397\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0992 - acc: 0.9448\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0959 - acc: 0.9487\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0928 - acc: 0.9525\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0898 - acc: 0.9572\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0870 - acc: 0.9608\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0843 - acc: 0.9652\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0818 - acc: 0.9673\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0794 - acc: 0.9703\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0771 - acc: 0.9726\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0750 - acc: 0.9752\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0729 - acc: 0.9773\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0710 - acc: 0.9803\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0692 - acc: 0.9813\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0674 - acc: 0.9831\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0658 - acc: 0.9843\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0642 - acc: 0.9861\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0627 - acc: 0.9870\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0613 - acc: 0.9888\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0599 - acc: 0.9894\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0586 - acc: 0.9903\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0574 - acc: 0.9896\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0562 - acc: 0.9917\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0551 - acc: 0.9914\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0540 - acc: 0.9926\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0530 - acc: 0.9931\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0520 - acc: 0.9927\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0510 - acc: 0.9942\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0501 - acc: 0.9935\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0492 - acc: 0.9944\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0484 - acc: 0.9945\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0475 - acc: 0.9948\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0468 - acc: 0.9956\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0460 - acc: 0.9939\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0453 - acc: 0.9954\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0446 - acc: 0.9949\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0439 - acc: 0.9954\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0432 - acc: 0.9963\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0426 - acc: 0.9958\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0420 - acc: 0.9951\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0414 - acc: 0.9951\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0408 - acc: 0.9959\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0402 - acc: 0.9968\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0397 - acc: 0.9957\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0391 - acc: 0.9972\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0386 - acc: 0.9961\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0381 - acc: 0.9969\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0376 - acc: 0.9963\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0372 - acc: 0.9960\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0367 - acc: 0.9968\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0363 - acc: 0.9976\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0358 - acc: 0.9969\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0354 - acc: 0.9985\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0350 - acc: 0.9969\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0346 - acc: 0.9984\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0342 - acc: 0.9966\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0338 - acc: 0.9982\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0335 - acc: 0.9975\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0331 - acc: 0.9969\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0327 - acc: 0.9982\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0324 - acc: 0.9977\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0321 - acc: 0.9977\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0317 - acc: 0.9987\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0314 - acc: 0.9985\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0311 - acc: 0.9982\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0308 - acc: 0.9973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x169d727c780>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_new, y_new, epochs=100, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems better. Lets look how it performs on our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.8051891e-01],\n",
       "       [8.8051891e-01],\n",
       "       [8.8051891e-01],\n",
       "       [5.7030648e-02],\n",
       "       [2.3332865e-04],\n",
       "       [9.0059268e-07]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We might want to check how the network performs on any data. For this, keras provides the evaluate function that will \n",
    "evaluate the loss and the metrics. So of course label (y) data is needed for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 30us/sample - loss: 0.0306 - acc: 0.9981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03061944737136364, 0.9981]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_new, y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we would normally do that on validation or test data not used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 20us/sample - loss: 0.0318 - acc: 0.9986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.031755273854732514, 0.9986]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val_new = np.random.random(size=(5000,2))\n",
    "y_val_new = np.zeros(5000, dtype=np.float32)\n",
    "y_val_new[x_val_new[:,1] > x_val_new[:,0]] = 1.0\n",
    "model.evaluate(x_val_new, y_val_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "It is essential to visualise the training process to see what is going on. In Keras, an easy method to do this is to use the history object that is returned from fit. It contains the metrics and the loss.\n",
    "\n",
    "We will also split our data into training and validation for this test. We rebuild the model, so that it is initialized again. Otherwise we would just continue with the weights from the previous fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/50\n",
      "7500/7500 [==============================] - 0s 63us/sample - loss: 0.2500 - acc: 0.4979 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 2/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 3/50\n",
      "7500/7500 [==============================] - 0s 14us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 4/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 5/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 6/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 7/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 8/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 9/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 10/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 11/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 12/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 13/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 14/50\n",
      "7500/7500 [==============================] - 0s 14us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 15/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 16/50\n",
      "7500/7500 [==============================] - 0s 16us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 17/50\n",
      "7500/7500 [==============================] - 0s 16us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 18/50\n",
      "7500/7500 [==============================] - 0s 16us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 19/50\n",
      "7500/7500 [==============================] - 0s 16us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 20/50\n",
      "7500/7500 [==============================] - 0s 16us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 21/50\n",
      "7500/7500 [==============================] - 0s 16us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 22/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 23/50\n",
      "7500/7500 [==============================] - 0s 16us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 24/50\n",
      "7500/7500 [==============================] - 0s 16us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 25/50\n",
      "7500/7500 [==============================] - 0s 14us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 26/50\n",
      "7500/7500 [==============================] - 0s 14us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 27/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 28/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 29/50\n",
      "7500/7500 [==============================] - 0s 14us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 30/50\n",
      "7500/7500 [==============================] - 0s 14us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 31/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 32/50\n",
      "7500/7500 [==============================] - 0s 16us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 33/50\n",
      "7500/7500 [==============================] - 0s 16us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 34/50\n",
      "7500/7500 [==============================] - 0s 16us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 35/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 36/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 37/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 38/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 39/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 40/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 41/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 42/50\n",
      "7500/7500 [==============================] - 0s 16us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 43/50\n",
      "7500/7500 [==============================] - 0s 16us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 44/50\n",
      "7500/7500 [==============================] - 0s 16us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 45/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 46/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 47/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 48/50\n",
      "7500/7500 [==============================] - 0s 14us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 49/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n",
      "Epoch 50/50\n",
      "7500/7500 [==============================] - 0s 15us/sample - loss: 0.2500 - acc: 0.5043 - val_loss: 0.2500 - val_acc: 0.5016\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(2, activation='relu', input_shape=[2]))\n",
    "model.add(keras.layers.Dense(2, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_new, y_new, validation_split=0.25, epochs=50, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x169da265358>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x169da41a898>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Correct loss function\n",
    "\n",
    "The loss function used above (mse) is not optimal. A better loss function would be the crossentropy. Change the network to use that loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Maximum of 4 colors\n",
    "\n",
    "Implement a network that will receive 4 colors and has to select one of them.\n",
    "\n",
    "This will require a change of the labels (y) that now take values of 0, 1, 2 or 3. However, networks do not use labels in that form directly for multi class classification, but use 1-hot encoded or categorical data instead.\n",
    "\n",
    "In keras there is a function `keras.utils.to_categorical` that can be used for that.\n",
    "\n",
    "The last layer in the network should then no longer be sigmoid, but the softmax function. And we need the multiclass form of the crossentropy function, which in keras is called `categorical_crossentropy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= np.random.random(size=(5000,4))\n",
    "y_train_label = np.argmax(x_train, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Implement a ML Network to learn trump from features\n",
    "\n",
    "We would like to train a network to get the trump from some features. (We could use the cards directly, but this is deep learning and we will see more of that in next lesson :-) )\n",
    "\n",
    "As features we can use the number of cards of a color as before and some of the features from last lecture. For keras all input features should be floating point numbers. Also we need numpy arrays and not pandas. To get the array from a panda, the property `values` can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "path_to_data = Path('data')\n",
    "# Import only a fraction of data for efficient testing\n",
    "data = pd.read_csv(path_to_data / '2018_10_18_trump.csv', header=None, nrows=1000)\n",
    "cards = [\n",
    "# Diamonds\n",
    "'DA','DK','DQ','DJ','D10','D9','D8','D7','D6',\n",
    "# Hearts\n",
    "'HA','HK','HQ','HJ','H10','H9','H8','H7','H6',\n",
    "# Spades\n",
    "'SA','SK','SQ','SJ','S10','S9','S8','S7','S6',\n",
    "# Clubs\n",
    "'CA','CK','CQ','CJ','C10','C9','C8','C7','C6'\n",
    "]\n",
    "\n",
    "# Forehand (yes = 1, no = 0)\n",
    "forehand = ['FH']\n",
    "\n",
    "user  = ['user']\n",
    "trump = ['trump']\n",
    "\n",
    "data.columns = cards + forehand + user + trump\n",
    "data.drop('user', axis='columns', inplace=True)\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue as follows:\n",
    "- Calculate features, \n",
    "- add them to the data set\n",
    "- drop the columns not used\n",
    "- convert to numpy array\n",
    "- build a network and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
