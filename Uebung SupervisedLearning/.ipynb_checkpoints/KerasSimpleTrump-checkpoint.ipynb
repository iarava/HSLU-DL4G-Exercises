{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for a simple neural network\n",
    "\n",
    "## Trump by maximum color (2 colors)\n",
    "\n",
    "The inputs to the network are the number of cards of each color. The network should learn to select the color with the largest number of cards of that color.\n",
    "\n",
    "For a simple example, let us assume that there are 5 cards in total for a player and only 2 colors.\n",
    "\n",
    "### Libraries\n",
    "\n",
    "We use the keras library for building, training and evaluating the network. A tutorial for keras can be found on (https://keras.io/) or https://www.tensorflow.org/guide/keras. There are different implementations of keras, here I will use the one build on tensorflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output function\n",
    "\n",
    "We have to encode the output somehow, for two classes, the simplest solution is a single variable that should be 0 if there are more cards of color 0 and 1 if there are more cards of color 1.\n",
    "\n",
    "### Training and label data.\n",
    "\n",
    "So we can prepare some training data. In this simple case, all the possible configurations are actually known.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 5.]\n",
      " [1. 4.]\n",
      " [2. 3.]\n",
      " [3. 2.]\n",
      " [4. 1.]\n",
      " [5. 0.]]\n",
      "[1. 1. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([\n",
    "    [0, 5],\n",
    "    [1, 4],\n",
    "    [2, 3],\n",
    "    [3, 2],\n",
    "    [4, 1],\n",
    "    [5, 0],\n",
    "], dtype=np.float32)\n",
    "y_train = np.array([1, 1, 1, 0, 0, 0,], dtype=np.float32)\n",
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation\n",
    "\n",
    "Input data can have different ranges. It is always a good idea (in other words absolutely essential) to normalize the input data. This is usually done into the range 0..1 or -1..1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  1. ]\n",
      " [0.2 0.8]\n",
      " [0.4 0.6]\n",
      " [0.6 0.4]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train / 5.0\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first network.\n",
    "\n",
    "We will start with a very simple network, where we connect the inputs directly to the output. So there will be 2 variables, the weights for the connection and the bias. The output function is a sigmoid, which takes values between 0 and 1.\n",
    "\n",
    "With keras, we first have to create the type of model we want (Sequential), and can then add layers. In the tensorflow implementation, we have to add the input_shape parameter in the first layer to tell it the format of the input. This does not include the batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lucar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid', input_shape=[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to compile the model and tell it what loss function and optimizer we want to have. We will take a mean squared error for loss function first. (This is actually not optimal and will be corrected in an exercise).\n",
    "\n",
    "Besides the loss, we usually want to look at some metrics. Here we choose accuracy, that measures how often the network makes the correct decision (see last lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lucar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print some details about the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[array([[0.42226315],\n",
      "       [0.19008565]], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "print(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either train one batch, or we can use fit to train repeatedly. The result from the training is the loss function and the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lucar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2731249, 0.5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_on_batch(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try to fit the data in minibatches multiple times. This will calculate the weights, so as to minimize the loss. We might not always get a good result in the first try and even this very simple network seems to need a large number of training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 0s 29ms/sample - loss: 0.2730 - acc: 0.5000\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 325us/sample - loss: 0.2729 - acc: 0.5000\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2727 - acc: 0.5000\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2726 - acc: 0.5000\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2725 - acc: 0.5000\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2723 - acc: 0.5000\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2722 - acc: 0.5000\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2721 - acc: 0.5000\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 325us/sample - loss: 0.2719 - acc: 0.5000\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2718 - acc: 0.5000\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 167us/sample - loss: 0.2717 - acc: 0.5000\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 331us/sample - loss: 0.2715 - acc: 0.5000\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2714 - acc: 0.5000\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 327us/sample - loss: 0.2713 - acc: 0.5000\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 338us/sample - loss: 0.2711 - acc: 0.5000\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 161us/sample - loss: 0.2710 - acc: 0.5000\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2709 - acc: 0.5000\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2707 - acc: 0.5000\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2706 - acc: 0.5000\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2705 - acc: 0.5000\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 167us/sample - loss: 0.2703 - acc: 0.5000\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2702 - acc: 0.5000\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 326us/sample - loss: 0.2701 - acc: 0.5000\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2699 - acc: 0.5000\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2698 - acc: 0.5000\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2697 - acc: 0.5000\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2695 - acc: 0.5000\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2694 - acc: 0.5000\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2693 - acc: 0.5000\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2691 - acc: 0.5000\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 160us/sample - loss: 0.2690 - acc: 0.5000\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2689 - acc: 0.5000\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 324us/sample - loss: 0.2688 - acc: 0.5000\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2686 - acc: 0.5000\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2685 - acc: 0.5000\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2684 - acc: 0.5000\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2682 - acc: 0.5000\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2681 - acc: 0.5000\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 158us/sample - loss: 0.2680 - acc: 0.5000\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2678 - acc: 0.5000\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2677 - acc: 0.5000\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2676 - acc: 0.5000\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2674 - acc: 0.5000\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 338us/sample - loss: 0.2673 - acc: 0.5000\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 160us/sample - loss: 0.2672 - acc: 0.5000\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2670 - acc: 0.5000\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2669 - acc: 0.5000\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 167us/sample - loss: 0.2668 - acc: 0.5000\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2667 - acc: 0.5000\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2665 - acc: 0.5000\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2664 - acc: 0.5000\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2663 - acc: 0.5000\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2661 - acc: 0.5000\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2660 - acc: 0.5000\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2659 - acc: 0.5000\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2657 - acc: 0.5000\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2656 - acc: 0.5000\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 162us/sample - loss: 0.2655 - acc: 0.5000\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2654 - acc: 0.5000\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2652 - acc: 0.5000\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2651 - acc: 0.5000\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2650 - acc: 0.5000\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 172us/sample - loss: 0.2648 - acc: 0.5000\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2647 - acc: 0.5000\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 339us/sample - loss: 0.2646 - acc: 0.5000\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2645 - acc: 0.5000\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2643 - acc: 0.5000\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2642 - acc: 0.5000\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2641 - acc: 0.5000\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 167us/sample - loss: 0.2639 - acc: 0.5000\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2638 - acc: 0.5000\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2637 - acc: 0.5000\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2636 - acc: 0.5000\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 328us/sample - loss: 0.2634 - acc: 0.5000\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2633 - acc: 0.5000\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2632 - acc: 0.5000\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2630 - acc: 0.5000\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2629 - acc: 0.5000\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2628 - acc: 0.5000\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2627 - acc: 0.5000\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 329us/sample - loss: 0.2625 - acc: 0.5000\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 161us/sample - loss: 0.2624 - acc: 0.5000\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2623 - acc: 0.5000\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2621 - acc: 0.5000\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2620 - acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2619 - acc: 0.5000\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2618 - acc: 0.5000\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 167us/sample - loss: 0.2616 - acc: 0.5000\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2615 - acc: 0.5000\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2614 - acc: 0.5000\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 338us/sample - loss: 0.2613 - acc: 0.5000\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2611 - acc: 0.5000\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2610 - acc: 0.5000\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2609 - acc: 0.5000\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2608 - acc: 0.5000\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2606 - acc: 0.5000\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 331us/sample - loss: 0.2605 - acc: 0.5000\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2604 - acc: 0.5000\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2602 - acc: 0.5000\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2601 - acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x169d6d6f7b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100, batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predict the values from the training value. Why are the results floating point number and not 0 or 1? Does the result seem likely?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5540702 ],\n",
       "       [0.5579984 ],\n",
       "       [0.56191945],\n",
       "       [0.56583273],\n",
       "       [0.56973785],\n",
       "       [0.57363427]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the found weights for each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense\n",
      "[array([[0.32953602],\n",
      "       [0.24997126]], dtype=float32), array([-0.03284146], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    print(layer.name)\n",
    "    print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we find the actual predictions? We use a threshold on the output of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train) > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A larger network\n",
    "\n",
    "Lets try a more complicated network with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(2, activation='relu', input_shape=[2]))\n",
    "model.add(keras.layers.Dense(2, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train it again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 0s 45ms/sample - loss: 0.3778 - acc: 0.5000\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3769 - acc: 0.5000\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 328us/sample - loss: 0.3760 - acc: 0.5000\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 328us/sample - loss: 0.3751 - acc: 0.5000\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 330us/sample - loss: 0.3742 - acc: 0.5000\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.3733 - acc: 0.5000\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3724 - acc: 0.5000\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3715 - acc: 0.5000\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.3706 - acc: 0.5000\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 329us/sample - loss: 0.3697 - acc: 0.5000\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3688 - acc: 0.5000\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 338us/sample - loss: 0.3679 - acc: 0.5000\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.3670 - acc: 0.5000\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3661 - acc: 0.5000\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3652 - acc: 0.5000\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3644 - acc: 0.5000\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 328us/sample - loss: 0.3635 - acc: 0.5000\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3626 - acc: 0.5000\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 498us/sample - loss: 0.3617 - acc: 0.5000\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3608 - acc: 0.5000\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3600 - acc: 0.5000\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3591 - acc: 0.5000\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 337us/sample - loss: 0.3582 - acc: 0.5000\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3573 - acc: 0.5000\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3565 - acc: 0.5000\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3556 - acc: 0.5000\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3547 - acc: 0.5000\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 327us/sample - loss: 0.3539 - acc: 0.5000\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 331us/sample - loss: 0.3530 - acc: 0.5000\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 328us/sample - loss: 0.3522 - acc: 0.5000\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3513 - acc: 0.5000\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3505 - acc: 0.5000\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3496 - acc: 0.5000\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 328us/sample - loss: 0.3488 - acc: 0.5000\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3479 - acc: 0.5000\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3471 - acc: 0.5000\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.3463 - acc: 0.5000\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 336us/sample - loss: 0.3454 - acc: 0.5000\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3446 - acc: 0.5000\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3438 - acc: 0.5000\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3430 - acc: 0.5000\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3421 - acc: 0.5000\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3413 - acc: 0.5000\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.3405 - acc: 0.5000\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3397 - acc: 0.5000\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3389 - acc: 0.5000\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 326us/sample - loss: 0.3377 - acc: 0.5000\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 336us/sample - loss: 0.3366 - acc: 0.5000\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 167us/sample - loss: 0.3355 - acc: 0.5000\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 326us/sample - loss: 0.3344 - acc: 0.5000\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3333 - acc: 0.5000\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3321 - acc: 0.5000\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3310 - acc: 0.5000\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3299 - acc: 0.5000\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 324us/sample - loss: 0.3288 - acc: 0.5000\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3278 - acc: 0.5000\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3267 - acc: 0.5000\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3256 - acc: 0.5000\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3245 - acc: 0.5000\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3234 - acc: 0.5000\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3224 - acc: 0.5000\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3213 - acc: 0.5000\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 338us/sample - loss: 0.3202 - acc: 0.5000\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 158us/sample - loss: 0.3192 - acc: 0.5000\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.3181 - acc: 0.5000\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3171 - acc: 0.5000\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 165us/sample - loss: 0.3161 - acc: 0.5000\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.3150 - acc: 0.5000\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 161us/sample - loss: 0.3140 - acc: 0.5000\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3130 - acc: 0.5000\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.3119 - acc: 0.5000\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 167us/sample - loss: 0.3109 - acc: 0.5000\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3099 - acc: 0.5000\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3089 - acc: 0.5000\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3079 - acc: 0.5000\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3069 - acc: 0.5000\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3059 - acc: 0.5000\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3049 - acc: 0.5000\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3039 - acc: 0.5000\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3030 - acc: 0.5000\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.3020 - acc: 0.5000\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3010 - acc: 0.5000\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3001 - acc: 0.5000\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2991 - acc: 0.5000\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2982 - acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2972 - acc: 0.5000\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2963 - acc: 0.5000\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2953 - acc: 0.5000\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2944 - acc: 0.5000\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2935 - acc: 0.5000\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2925 - acc: 0.5000\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2916 - acc: 0.5000\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2907 - acc: 0.5000\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2898 - acc: 0.5000\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2889 - acc: 0.5000\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 160us/sample - loss: 0.2880 - acc: 0.5000\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 324us/sample - loss: 0.2871 - acc: 0.5000\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 0.2862 - acc: 0.5000\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2854 - acc: 0.5000\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 166us/sample - loss: 0.2845 - acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x169d577b438>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100, batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not necessarly better, how does the prediction look now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33950227],\n",
       "       [0.40222698],\n",
       "       [0.46638152],\n",
       "       [0.47019863],\n",
       "       [0.4563381 ],\n",
       "       [0.44039917]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.layers.core.Dense object at 0x00000169D6EB34A8>\n",
      "[array([[-0.30224428,  0.96389234],\n",
      "       [ 0.9328564 , -0.09588786]], dtype=float32), array([-0.15454654,  0.01976662], dtype=float32)]\n",
      "<tensorflow.python.keras.layers.core.Dense object at 0x00000169D6EB3630>\n",
      "[array([[ 0.80477417,  0.2516636 ],\n",
      "       [-0.17768416,  0.32756498]], dtype=float32), array([-0.14752015, -0.02973005], dtype=float32)]\n",
      "<tensorflow.python.keras.layers.core.Dense object at 0x00000169D6EB3898>\n",
      "[array([[-1.1344988],\n",
      "       [-0.9282603]], dtype=float32), array([0.03195744], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    print(layer)\n",
    "    print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger network, does not seem to work better as the simpler one. Or is it maybe not large enough?\n",
    "\n",
    "The problem is not the network, but the data, we just do not have enough data. So lets try to make up some more data artificially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.random.random(size=(10000,2))\n",
    "y_new = np.zeros(10000, dtype=np.float32)\n",
    "condition = (x_new[:,1] > x_new[:,0])\n",
    "y_new[condition] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2536 - acc: 0.5565\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2425 - acc: 0.6022\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2378 - acc: 0.6458\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2344 - acc: 0.6801\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2315 - acc: 0.7117\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2286 - acc: 0.7227\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2257 - acc: 0.7321\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2226 - acc: 0.7407\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2194 - acc: 0.7526\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2161 - acc: 0.7574\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2125 - acc: 0.7670\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2087 - acc: 0.7744\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.2047 - acc: 0.7831\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.2004 - acc: 0.7899\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1960 - acc: 0.8008\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1914 - acc: 0.8113\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1866 - acc: 0.8171\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1817 - acc: 0.8267\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1767 - acc: 0.8353\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1716 - acc: 0.8431\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1665 - acc: 0.8511\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1613 - acc: 0.8592\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1561 - acc: 0.8661\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1510 - acc: 0.8734\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.1460 - acc: 0.8817\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1410 - acc: 0.8892\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1362 - acc: 0.8954\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.1315 - acc: 0.9015\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1269 - acc: 0.9065\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1225 - acc: 0.9147\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1182 - acc: 0.9200\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1140 - acc: 0.9248\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1101 - acc: 0.9299\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1063 - acc: 0.9357\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.1027 - acc: 0.9397\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0992 - acc: 0.9448\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0959 - acc: 0.9487\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0928 - acc: 0.9525\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0898 - acc: 0.9572\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0870 - acc: 0.9608\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0843 - acc: 0.9652\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0818 - acc: 0.9673\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0794 - acc: 0.9703\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0771 - acc: 0.9726\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0750 - acc: 0.9752\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0729 - acc: 0.9773\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0710 - acc: 0.9803\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0692 - acc: 0.9813\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0674 - acc: 0.9831\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0658 - acc: 0.9843\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0642 - acc: 0.9861\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0627 - acc: 0.9870\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0613 - acc: 0.9888\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0599 - acc: 0.9894\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0586 - acc: 0.9903\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0574 - acc: 0.9896\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0562 - acc: 0.9917\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0551 - acc: 0.9914\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0540 - acc: 0.9926\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0530 - acc: 0.9931\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0520 - acc: 0.9927\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0510 - acc: 0.9942\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0501 - acc: 0.9935\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0492 - acc: 0.9944\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0484 - acc: 0.9945\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0475 - acc: 0.9948\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0468 - acc: 0.9956\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0460 - acc: 0.9939\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0453 - acc: 0.9954\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0446 - acc: 0.9949\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0439 - acc: 0.9954\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0432 - acc: 0.9963\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0426 - acc: 0.9958\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0420 - acc: 0.9951\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0414 - acc: 0.9951\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0408 - acc: 0.9959\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0402 - acc: 0.9968\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0397 - acc: 0.9957\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0391 - acc: 0.9972\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0386 - acc: 0.9961\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0381 - acc: 0.9969\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0376 - acc: 0.9963\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0372 - acc: 0.9960\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0367 - acc: 0.9968\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0363 - acc: 0.9976\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0358 - acc: 0.9969\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0354 - acc: 0.9985\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0350 - acc: 0.9969\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0346 - acc: 0.9984\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0342 - acc: 0.9966\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0338 - acc: 0.9982\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0335 - acc: 0.9975\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0331 - acc: 0.9969\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0327 - acc: 0.9982\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0324 - acc: 0.9977\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 0s 15us/sample - loss: 0.0321 - acc: 0.9977\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 0s 14us/sample - loss: 0.0317 - acc: 0.9987\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0314 - acc: 0.9985\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 0s 12us/sample - loss: 0.0311 - acc: 0.9982\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 0s 13us/sample - loss: 0.0308 - acc: 0.9973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x169d727c780>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_new, y_new, epochs=100, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems better. Lets look how it performs on our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We might want to check how the network performs on any data. For this, keras provides the evaluate function that will \n",
    "evaluate the loss and the metrics. So of course label (y) data is needed for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_new, y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we would normally do that on validation or test data not used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_new = np.random.random(size=(5000,2))\n",
    "y_val_new = np.zeros(5000, dtype=np.float32)\n",
    "y_val_new[x_val_new[:,1] > x_val_new[:,0]] = 1.0\n",
    "model.evaluate(x_val_new, y_val_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "It is essential to visualise the training process to see what is going on. In Keras, an easy method to do this is to use the history object that is returned from fit. It contains the metrics and the loss.\n",
    "\n",
    "We will also split our data into training and validation for this test. We rebuild the model, so that it is initialized again. Otherwise we would just continue with the weights from the previous fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(2, activation='relu', input_shape=[2]))\n",
    "model.add(keras.layers.Dense(2, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_new, y_new, validation_split=0.25, epochs=50, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Correct loss function\n",
    "\n",
    "The loss function used above (mse) is not optimal. A better loss function would be the crossentropy. Change the network to use that loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Maximum of 4 colors\n",
    "\n",
    "Implement a network that will receive 4 colors and has to select one of them.\n",
    "\n",
    "This will require a change of the labels (y) that now take values of 0, 1, 2 or 3. However, networks do not use labels in that form directly for multi class classification, but use 1-hot encoded or categorical data instead.\n",
    "\n",
    "In keras there is a function `keras.utils.to_categorical` that can be used for that.\n",
    "\n",
    "The last layer in the network should then no longer be sigmoid, but the softmax function. And we need the multiclass form of the crossentropy function, which in keras is called `categorical_crossentropy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= np.random.random(size=(5000,4))\n",
    "y_train_label = np.argmax(x_train, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Implement a ML Network to learn trump from features\n",
    "\n",
    "We would like to train a network to get the trump from some features. (We could use the cards directly, but this is deep learning and we will see more of that in next lesson :-) )\n",
    "\n",
    "As features we can use the number of cards of a color as before and some of the features from last lecture. For keras all input features should be floating point numbers. Also we need numpy arrays and not pandas. To get the array from a panda, the property `values` can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "path_to_data = Path('data')\n",
    "# Import only a fraction of data for efficient testing\n",
    "data = pd.read_csv(path_to_data / '2018_10_18_trump.csv', header=None, nrows=1000)\n",
    "cards = [\n",
    "# Diamonds\n",
    "'DA','DK','DQ','DJ','D10','D9','D8','D7','D6',\n",
    "# Hearts\n",
    "'HA','HK','HQ','HJ','H10','H9','H8','H7','H6',\n",
    "# Spades\n",
    "'SA','SK','SQ','SJ','S10','S9','S8','S7','S6',\n",
    "# Clubs\n",
    "'CA','CK','CQ','CJ','C10','C9','C8','C7','C6'\n",
    "]\n",
    "\n",
    "# Forehand (yes = 1, no = 0)\n",
    "forehand = ['FH']\n",
    "\n",
    "user  = ['user']\n",
    "trump = ['trump']\n",
    "\n",
    "data.columns = cards + forehand + user + trump\n",
    "data.drop('user', axis='columns', inplace=True)\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue as follows:\n",
    "- Calculate features, \n",
    "- add them to the data set\n",
    "- drop the columns not used\n",
    "- convert to numpy array\n",
    "- build a network and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
