{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given an excerpt of the log data from the [Swisslos](https://www.swisslos.ch) Jass servers collected between October 2017 and April 2018. In the Swiss card game [Jass](https://en.wikipedia.org/wiki/Jass), at the beginning of each round just after the cards have been dealt, the current player announces her choice of the trump mode from the options: diamonds, hearts, spades, club, obe-abe, une-ufe and push (only available when forehand). Use this information from human players to train a machine learning classifier in trump mode prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import only a fraction of data for efficient testing\n",
    "# data = pd.read_csv('data/2018_10_18_trump.csv', header=None, nrows=1000)\n",
    "\n",
    "# Import all data in the final run\n",
    "data = pd.read_csv(\"data/2018_10_18_trump_Test.csv\", header=None)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.groupby(37).size().reset_index(name='counts')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[(df[\"counts\"]>500)].index)[37]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(df)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of rows and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename Columns for better Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = [\n",
    "# Diamonds\n",
    "'DA','DK','DQ','DJ','D10','D9','D8','D7','D6',\n",
    "# Hearts\n",
    "'HA','HK','HQ','HJ','H10','H9','H8','H7','H6',\n",
    "# Spades\n",
    "'SA','SK','SQ','SJ','S10','S9','S8','S7','S6',\n",
    "# Clubs\n",
    "'CA','CK','CQ','CJ','C10','C9','C8','C7','C6'\n",
    "]\n",
    "\n",
    "# Forehand (yes = 1, no = 0)\n",
    "forehand = ['FH']\n",
    "\n",
    "user  = ['user']\n",
    "trump = ['trump']\n",
    "\n",
    "data.columns = cards + forehand + user + trump\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove User Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on you may want to keep this information. For now, we remove it just to avoid mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('user', axis='columns', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.trump = data.trump.astype('category')\n",
    "data[cards + forehand] = data[cards + forehand].astype(bool)\n",
    "#data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename Target Category Values for better Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward compatibility: Value 10 for PUSH was used in an older version by Swisslos\n",
    "\n",
    "data.trump.cat.rename_categories({0: 'DIAMONDS', 1: 'HEARTS', 2: 'SPADES', 3:'CLUBS',\n",
    "                                  4: 'OBE_ABE', 5: 'UNE_UFE', 6: 'PUSH', 10: 'PUSH'}, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Sanity Check\n",
    "\n",
    "Each row in the data now corresponds to one hand of a player plus her trump mode selection. Verify that each hand contains exactly 9 cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "data_cards = data[cards]\n",
    "data = data[data_cards.sum(axis=1) == 9]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Statistical Fingerprint\n",
    "\n",
    "Swisslos claims to distribute cards randomly.\n",
    "* Verify that all card features roughly have the same mean and standard deviation.\n",
    "* Over a **large number of rounds**, we would expect similar percentages for diamonds, hearts, spades and clubs as selected Trump. Verify this claim.\n",
    "\n",
    "Hint: you may want to use [np.allclose(...)](https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html) in combination with an assert statement for automated checking.\n",
    "\n",
    "#### A note on randomness - not part of exercises: \n",
    "\n",
    "The above test is obviously a very poor guarantee of randomness. However, we perform such tests from the viewpoint of data quality rather than fairness. If you want to challenge the random number generator of Swisslos, use the NIST or Diehard test suite for randomness. Read more about random number tests <a href='https://gerhardt.ch/random.php'>here</a>. We have not done this ourself and keep believing that the provider of Swiss lottery use a reasonable source of randomness even for online games :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "diamonds = np.where(data.trump == 'DIAMONDS', 1, 0)\n",
    "hearts = np.where(data.trump == 'HEARTS', 1, 0)\n",
    "spades = np.where(data.trump == 'SPADES', 1, 0)\n",
    "clubs = np.where(data.trump == 'CLUBS', 1, 0)\n",
    "obe_abe = np.where(data.trump == 'OBE_ABE', 1, 0)\n",
    "une_ufe = np.where(data.trump == 'UNE_UFE', 1, 0)\n",
    "push = np.where(data.trump == 'PUSH', 1, 0)\n",
    "print(diamonds.mean())\n",
    "print(hearts.mean())\n",
    "print(spades.mean())\n",
    "print(clubs.mean())\n",
    "print(obe_abe.mean())\n",
    "print(une_ufe.mean())\n",
    "print(push.mean())\n",
    "print(diamonds.std())\n",
    "print(hearts.std())\n",
    "print(spades.std())\n",
    "print(clubs.std())\n",
    "print(obe_abe.std())\n",
    "print(une_ufe.std())\n",
    "print(push.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.trump.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data[((data.trump == 'PUSH') & (data.index % 3 != 0))].index)  \n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.trump.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds = np.where(data.trump == 'DIAMONDS', 1, 0)\n",
    "hearts = np.where(data.trump == 'HEARTS', 1, 0)\n",
    "spades = np.where(data.trump == 'SPADES', 1, 0)\n",
    "clubs = np.where(data.trump == 'CLUBS', 1, 0)\n",
    "obe_abe = np.where(data.trump == 'OBE_ABE', 1, 0)\n",
    "une_ufe = np.where(data.trump == 'UNE_UFE', 1, 0)\n",
    "push = np.where(data.trump == 'PUSH', 1, 0)\n",
    "print(diamonds.mean())\n",
    "print(hearts.mean())\n",
    "print(spades.mean())\n",
    "print(clubs.mean())\n",
    "print(obe_abe.mean())\n",
    "print(une_ufe.mean())\n",
    "print(push.mean())\n",
    "print(diamonds.std())\n",
    "print(hearts.std())\n",
    "print(spades.std())\n",
    "print(clubs.std())\n",
    "print(obe_abe.std())\n",
    "print(une_ufe.std())\n",
    "print(push.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you explain ...\n",
    "* the difference between Une-Ufe and Obe-Abe ?\n",
    "* the difference between {spades, clubs} and {hearts, diamonds}\n",
    "\n",
    "The fact that the values of the response are not distributed evenly has implications on the train-test split in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "Split the available data into a training and a test set and put the test set aside for the final model evaluation. Use 20% of the data for testing, and set the random_state to 42 for reproducability. Since we found out that the classes are not distributed evenly, you need to stratify. Also, be aware that train_test_split() shuffles the data by default, which is what we want here (but not always, e.g. not in case of time series).\n",
    "\n",
    "Your result should be the four variables *X_train, X_test, y_train, y_test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_columns = cards + forehand\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[feature_columns], data.trump, test_size=0.2,\n",
    "                                                    stratify=data.trump, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline with a Logistic Regression Classifier\n",
    "\n",
    "We create a (linear) logistic regression classifier and evaluate it with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Note that we do not use the test set here !\n",
    "result = cross_val_score(classifier, X_train, y_train, cv=5, n_jobs=-1)\n",
    "\n",
    "print(\"Mean accuracy over 5 folds is {:.4}\".format(np.mean(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Stochastic Gradient Descent\n",
    "\n",
    "Machine learning classifiers internally use an optimization algorithm. Stochastic gradient descent is a stochastic \n",
    "approximation that generally is more efficient but with a slightly lower convergence rate. Change the code in the previous cell to a logistic regression with stochastic gradient descent. Use [SGDClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) with parameters *loss='log'* and *penalty='None'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# todo\n",
    "classifier = SGDClassifier(loss='log', penalty='None')\n",
    "\n",
    "# Note that we do not use the test set here !\n",
    "result = cross_val_score(classifier, X_train, y_train, cv=5, n_jobs=-1)\n",
    "\n",
    "print(\"Mean accuracy over 5 folds is {:.4}\".format(np.mean(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "\n",
    "Learning curves are an important diagnostic instrument in machine learning. A cross-validation generator splits the whole dataset k times into training and test data. Subsets of the training set with increasing sizes will be used to train the estimator and a score for each training subset size on the validation set will be computed. Afterwards, the scores will be averaged over all k runs for each training subset size.\n",
    "\n",
    "As a rule of thumb, a gap between the training and validation curves indicates that using more data would improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(SGDClassifier(loss='log', penalty='None'), \n",
    "                                                         X_train, y_train, n_jobs=-1,\n",
    "                                                         train_sizes=np.linspace(0.1, 1.0, 50), cv=10,\n",
    "                                                         exploit_incremental_learning=True, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.DataFrame({'train':train_scores.mean(axis=1), 'validation':valid_scores.mean(axis=1)}, index=train_sizes)\n",
    "f, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.set_xlabel('#samples')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('SGD')\n",
    "plot_data.plot(ax=ax)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create a Random Forest Classifier\n",
    "\n",
    "Now create a random forest classifier and evaluate it with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# todo\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# Note that we do not use the test set here !\n",
    "result = cross_val_score(classifier, X_train, y_train, cv=5, n_jobs=-1)\n",
    "\n",
    "print(\"Mean accuracy over 5 folds is {:.4}\".format(np.mean(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: List and Plot Feature Importances\n",
    "\n",
    "Tree classsifiers (such as decision trees and random forests) can conveniently list feature importances. They use feature values to split the training set, whereas more important features are used higher in the tree(s). Use the *feature_importances_* attribute of a trained RandomForestClassifier to obtain a sorted list of the most important features. Plot feature importances in a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "classifier.fit(X_train,y_train)\n",
    "classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search over multiple Classifiers and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Classifier names\n",
    "names = [\n",
    "    \"Logistic Regression\",\n",
    "    \"Random Forest\"\n",
    "]\n",
    "\n",
    "# Classifiers\n",
    "classifiers = [\n",
    "    LogisticRegression(solver='lbfgs', multi_class='multinomial'), # set these to avoid a FutureWarning with scikit-lean < 0.22\n",
    "    RandomForestClassifier(n_estimators=200)\n",
    "]\n",
    "\n",
    "# Hyperparameter grid to search per classifier\n",
    "parameters = [\n",
    "    {'C': [0.5, 1.0, 2.0]},\n",
    "    {'max_depth': [5, 10]}\n",
    "]\n",
    "\n",
    "list(zip(names, parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, classifier, params in zip(names, classifiers, parameters):\n",
    "    print(\"Grid search for {}\".format(name))\n",
    "    gs = GridSearchCV(classifier, param_grid=params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Best accuracy score found: {:.3f}\\n\".format(gs.best_score_))\n",
    "    results.append([name, gs.best_score_, gs.best_estimator_])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: If *refit=True* (the default), the best estimator is made available at the *best_estimator_* attribute of gs and permits using *.predict()* directly on this GridSearchCV instance. Note, however, that the above code runs grid search separately for each classifier, and so gs now only contains the last estimator of the list, which may or may not be the best one. So don't use gs to predict blindly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Extend Grid Search by adding other Classifiers and Hyperparameters\n",
    "\n",
    "You may want to experiment with a [nearest neighbors classifier](http://scikit-learn.org/stable/modules/neighbors.html) or a [support vector machine](http://scikit-learn.org/stable/modules/svm.html) or a [neural network](http://scikit-learn.org/stable/modules/neural_networks_supervised.html) or ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Add interactions to Logistic Regression\n",
    "\n",
    "A linear classifier such as logistic regression cannot model dependencies between features (things like \"if your hand has both heart jack and heart nine, then choose heart as trump mode\"). But features like these called *interactions* can be added manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get the score without interactions\n",
    "clf = LogisticRegression(C=0.8, solver='lbfgs', multi_class='multinomial')\n",
    "result = cross_val_score(clf, X_train, y_train, cv=5, n_jobs=-1)\n",
    "print(\"Mean accuracy over 5 folds is {:.4}\".format(np.mean(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the four colors, add one interaction term for the combination jack and nine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for color in 'DHSC':\n",
    "    # Jack and nine combination\n",
    "    new_col = '{}_J9'.format(color)\n",
    "    data[new_col]  = data['{}J'.format(color)] & data['{}9'.format(color)]\n",
    "    feature_columns.append(new_col)\n",
    "    \n",
    "    # Exercise: Add other features here such as the combination of Ace-King-Queen (Dreiblatt).\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must recreate training and test sets with the new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[feature_columns], data.trump, test_size=0.2,\n",
    "                                                    stratify=data.trump, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check  if the additional columns improve the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=0.8, solver='lbfgs', multi_class='multinomial')\n",
    "result = cross_val_score(clf, X_train, y_train, cv=5, n_jobs=-1)\n",
    "print(\"Mean accuracy over 5 folds is {:.4}\".format(np.mean(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Better Accuracy with less Data ?\n",
    "\n",
    "At the beginning we removed the user ID from our data set. Use this information to develop a heuristic that seggregates between *good* and *bad* players. Can you improve accuracy by using only the data from *good* players?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no solution available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: The final Score\n",
    "\n",
    "Use the best setting you could find, train the classifier on all training data and evaluate on the yet unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably not the best :-)\n",
    "clf = LogisticRegression(C=0.5, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
